# Programming Hive by O'Reilly

â€‹		*Nowadays, Hive is one of the most popular big data warehouse solution despite the rise of [Spark](https://spark.apache.org/). As a beginner, I would say Hive is the easiest entry point for anyone who wants to dive in Hadoop's ecosystem. Personally, I use Hive extensively during my internships and I always keep O'Reilly's [Programming Hive](https://www.amazon.com/Programming-Hive-Warehouse-Language-Hadoop/dp/1449319335) at my desk as a reference. Although after this book was published at 2013, Hive has been developing rapidly evey year and  some examples in the book are no longer valid, overall this is still a good book as the first tutorial to get to know Hive. Here are my notes while reading this book.*



## Chapter 1 - Overview

- The Hive modules, [this is from Hive wiki.](https://cwiki.apache.org/confluence/display/Hive/Design) Todo: read article again for better understanding

![](./system_architecture.png)

"Figure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table's location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9)."



## Chapter 2 - Configuration

- Modes: 

  - Local Mode:

    - in local filesystem, all tasks (Mapper, Reducer) are in the same process

  - Psedodistributed Mode: 

    - in HDFS, each task are in seperate processes, managed by Job Tracker 

  - Distributed Mode

    Note this is **per-query** **execution** sytle rather than **deployment** style: even in distributed mode, a query could be run in local mode.

- hive-site.xml

  ```xml
  <property>
    <name>hive.metastore.warehouse.dir</name>
  	<value>/user/hive/warehouse</value>
  </property>
  <property>
    <name>hive.metastore.local</name>
    <value>true</value> <!-- almost changed to false -->
  </property>
  ```

- .hiverc

  ```shell
  hive -i xxx.hiverc # initialization script
  ```

- metastore using JDBC: can be configured in hive-site.xml

- Hive variables

  ```shell
  hive --hivevar key=value #1
  hive --hiveconf key=value #2
  
  hive --hivevar ab='aa' -e "select '${hivevar:ab}'" ???
  # 3. env
  # 4. system
  ```

- hive command to write to local file (important)

  ```shell
  hive --silent -e "select * from app_persona_age" >> ./output.txt
  ```

- Tab is autocomplete, therefore **do not** start a line with a Tab in Hive CLI (important), otherwise will see the following

  ```
  Display all 478 possibilities? (y or n)
  ```

  